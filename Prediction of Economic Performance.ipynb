{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting CERFRANCE EBE Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import dataiku\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import dataiku.core.pandasutils as pdu\n",
    "from dataiku.doctor.preprocessing import PCA\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And tune pandas display options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 3000)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing base data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get our machine learning dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the preparation that you defined. You should not modify this.\n",
    "preparation_steps = []\n",
    "preparation_output_schema = {u'userModified': False, u'columns': [{u'type': u'string', u'name': u'CERFRANCE'}, {u'type': u'double', u'name': u'charg123_avg'}, {u'type': u'double', u'name': u'charg123_stddev'}, {u'type': u'double', u'name': u'charg126_avg'}, {u'type': u'double', u'name': u'charg126_stddev'}, {u'type': u'double', u'name': u'marg469_avg'}, {u'type': u'double', u'name': u'marg469_stddev'}, {u'type': u'double', u'name': u'marg747_avg'}, {u'type': u'double', u'name': u'marg747_stddev'}, {u'type': u'double', u'name': u'marg750_avg'}, {u'type': u'double', u'name': u'marg750_stddev'}, {u'type': u'double', u'name': u'1_charg123'}, {u'type': u'double', u'name': u'1_charg126'}, {u'type': u'double', u'name': u'1_marg469'}, {u'type': u'double', u'name': u'1_marg747'}, {u'type': u'double', u'name': u'1_marg750'}, {u'type': u'bigint', u'name': u'traj_ebe_cice_new'}, {u'type': u'double', u'name': u'evolution_ddb_agri_n_n3'}, {u'type': u'double', u'name': u'evolution_n_n3_ecofi23'}, {u'type': u'bigint', u'name': u'notif_dev_new'}, {u'type': u'double', u'name': u'evolution_ddb_acs_n_n3'}]}\n",
    "\n",
    "ml_dataset_handle = dataiku.Dataset('RECUP_ECO_ALERT_NOTIFICATION_prepared')\n",
    "ml_dataset_handle.set_preparation_steps(preparation_steps, preparation_output_schema)\n",
    "%time ml_dataset = ml_dataset_handle.get_dataframe(limit = 100000)\n",
    "\n",
    "print ('Base data has %i rows and %i columns' % (ml_dataset.shape[0], ml_dataset.shape[1]))\n",
    "# Five first records\",\n",
    "ml_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variable selection\n",
    "ml_dataset = ml_dataset[[u'marg469_avg', u'1_marg469', u'charg126_stddev', u'1_charg126', u'marg469_stddev', u'charg126_avg', u'charg123_stddev', u'marg750_stddev', u'1_charg123', u'1_marg747', u'traj_ebe_cice_new', u'marg747_avg', u'charg123_avg', u'marg747_stddev', u'1_marg750', u'marg750_avg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first coerce categorical columns into unicode, numerical features into floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# astype('unicode') does not work as expected\n",
    "\n",
    "def coerce_to_unicode(x):\n",
    "    if sys.version_info < (3, 0):\n",
    "        if isinstance(x, str):\n",
    "            return unicode(x,'utf-8')\n",
    "        else:\n",
    "            return unicode(x)\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "\n",
    "categorical_features = []\n",
    "numerical_features = [u'marg469_avg', u'1_marg469', u'charg126_stddev', u'1_charg126', u'marg469_stddev', u'charg126_avg', u'charg123_stddev', u'marg750_stddev', u'1_charg123', u'1_marg747', u'marg747_avg', u'charg123_avg', u'marg747_stddev', u'1_marg750', u'marg750_avg']\n",
    "text_features = []\n",
    "from dataiku.doctor.utils import datetime_to_epoch\n",
    "for feature in categorical_features:\n",
    "    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)\n",
    "for feature in text_features:\n",
    "    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)\n",
    "for feature in numerical_features:\n",
    "    if ml_dataset[feature].dtype == np.dtype('M8[ns]') or (hasattr(ml_dataset[feature].dtype, 'base') and ml_dataset[feature].dtype.base == np.dtype('M8[ns]')):\n",
    "        ml_dataset[feature] = datetime_to_epoch(ml_dataset[feature])\n",
    "    else:\n",
    "        ml_dataset[feature] = ml_dataset[feature].astype('double')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to handle the target variable and store it in a new variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {u'1': 1, u'2': 0}\n",
    "ml_dataset['__target__'] = ml_dataset['traj_ebe_cice_new'].map(str).map(target_map)\n",
    "del ml_dataset['traj_ebe_cice_new']\n",
    "\n",
    "\n",
    "# Remove rows for which the target is unknown.\n",
    "ml_dataset = ml_dataset[~ml_dataset['__target__'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = pdu.split_train_valid(ml_dataset, prop=0.8)\n",
    "print ('Train data has %i rows and %i columns' % (train.shape[0], train.shape[1]))\n",
    "print ('Test data has %i rows and %i columns' % (test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do at the features level is to handle the missing values.\n",
    "Let's reuse the settings defined in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows_when_missing = []\n",
    "impute_when_missing = [{'impute_with': u'MEAN', 'feature': u'marg469_avg'}, {'impute_with': u'MEAN', 'feature': u'1_marg469'}, {'impute_with': u'MEAN', 'feature': u'charg126_stddev'}, {'impute_with': u'MEAN', 'feature': u'1_charg126'}, {'impute_with': u'MEAN', 'feature': u'marg469_stddev'}, {'impute_with': u'MEAN', 'feature': u'charg126_avg'}, {'impute_with': u'MEAN', 'feature': u'charg123_stddev'}, {'impute_with': u'MEAN', 'feature': u'marg750_stddev'}, {'impute_with': u'MEAN', 'feature': u'1_charg123'}, {'impute_with': u'MEAN', 'feature': u'1_marg747'}, {'impute_with': u'MEAN', 'feature': u'marg747_avg'}, {'impute_with': u'MEAN', 'feature': u'charg123_avg'}, {'impute_with': u'MEAN', 'feature': u'marg747_stddev'}, {'impute_with': u'MEAN', 'feature': u'1_marg750'}, {'impute_with': u'MEAN', 'feature': u'marg750_avg'}]\n",
    "\n",
    "# Features for which we drop rows with missing values\"\n",
    "for feature in drop_rows_when_missing:\n",
    "    train = train[train[feature].notnull()]\n",
    "    test = test[test[feature].notnull()]\n",
    "    print ('Dropped missing records in %s' % feature)\n",
    "\n",
    "# Features for which we impute missing values\"\n",
    "for feature in impute_when_missing:\n",
    "    if feature['impute_with'] == 'MEAN':\n",
    "        v = train[feature['feature']].mean()\n",
    "    elif feature['impute_with'] == 'MEDIAN':\n",
    "        v = train[feature['feature']].median()\n",
    "    elif feature['impute_with'] == 'CREATE_CATEGORY':\n",
    "        v = 'NULL_CATEGORY'\n",
    "    elif feature['impute_with'] == 'MODE':\n",
    "        v = train[feature['feature']].value_counts().index[0]\n",
    "    elif feature['impute_with'] == 'CONSTANT':\n",
    "        v = feature['value']\n",
    "    train[feature['feature']] = train[feature['feature']].fillna(v)\n",
    "    test[feature['feature']] = test[feature['feature']].fillna(v)\n",
    "    print ('Imputed missing values in feature %s with value %s' % (feature['feature'], coerce_to_unicode(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now handle the categorical features (still using the settings defined in Models):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rescale numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_features = {u'marg469_avg': u'AVGSTD', u'1_marg469': u'AVGSTD', u'charg126_stddev': u'AVGSTD', u'marg469_stddev': u'AVGSTD', u'charg123_avg': u'AVGSTD', u'charg126_avg': u'AVGSTD', u'charg123_stddev': u'AVGSTD', u'marg750_stddev': u'AVGSTD', u'1_charg123': u'AVGSTD', u'1_charg126': u'AVGSTD', u'marg747_avg': u'AVGSTD', u'1_marg747': u'AVGSTD', u'marg747_stddev': u'AVGSTD', u'1_marg750': u'AVGSTD', u'marg750_avg': u'AVGSTD'}\n",
    "for (feature_name, rescale_method) in rescale_features.items():\n",
    "    if rescale_method == 'MINMAX':\n",
    "        _min = train[feature_name].min()\n",
    "        _max = train[feature_name].max()\n",
    "        scale = _max - _min\n",
    "        shift = _min\n",
    "    else:\n",
    "        shift = train[feature_name].mean()\n",
    "        scale = train[feature_name].std()\n",
    "    if scale == 0.:\n",
    "        del train[feature_name]\n",
    "        del test[feature_name]\n",
    "        print ('Feature %s was dropped because it has no variance' % feature_name)\n",
    "    else:\n",
    "        print ('Rescaled %s' % feature_name)\n",
    "        train[feature_name] = (train[feature_name] - shift).astype(np.float64) / scale\n",
    "        test[feature_name] = (test[feature_name] - shift).astype(np.float64) / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before actually creating our model, we need to split the datasets into their features and labels parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop('__target__', axis=1)\n",
    "test_X = test.drop('__target__', axis=1)\n",
    "\n",
    "train_Y = np.array(train['__target__'])\n",
    "test_Y = np.array(test['__target__'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally create our model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(\n",
    "                    n_estimators=30,\n",
    "                    random_state=1337,\n",
    "                    max_depth=10,\n",
    "                    min_samples_leaf=3,\n",
    "                    verbose=2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training my Model\n",
    "\n",
    "%time clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now trained, we can apply it to our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation on the Test Set\n",
    "\n",
    "%time _predictions = clf.predict(test_X)\n",
    "%time _probas = clf.predict_proba(test_X)\n",
    "predictions = pd.Series(data=_predictions, index=test_X.index, name='predicted_value')\n",
    "cols = [\n",
    "    u'probability_of_value_%s' % label\n",
    "    for (_, label) in sorted([(int(target_map[label]), label) for label in target_map])\n",
    "]\n",
    "probabilities = pd.DataFrame(data=_probas, index=test_X.index, columns=cols)\n",
    "\n",
    "# Build scored dataset\n",
    "results_test = test_X.join(predictions, how='left')\n",
    "results_test = results_test.join(probabilities, how='left')\n",
    "results_test = results_test.join(test['__target__'], how='left')\n",
    "results_test = results_test.rename(columns= {'__target__': 'traj_ebe_cice_new'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataiku.doctor.utils.metrics import mroc_auc_score\n",
    "test_Y_ser = pd.Series(test_Y)\n",
    "print ('AUC value:', mroc_auc_score(test_Y_ser, _probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reversing the mapping to display initial labels\n",
    "inv_map = { target_map[label] : label for label in target_map}\n",
    "predictions.map(inv_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "Predicting traj_ebe_cice_new in RECUP_ECO_ALERT_NOTIFICATION_prepared"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
