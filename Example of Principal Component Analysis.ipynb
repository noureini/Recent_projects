{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) on EXPLORATION_NOTIF_CONSEIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and dataset loading <a id=\"setup\" /> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import dataiku                               # Access to Dataiku datasets\n",
    "import pandas as pd, numpy as np             # Data manipulation \n",
    "from sklearn.decomposition import PCA        # The main algorithm\n",
    "from matplotlib import pyplot as plt         # Graphing\n",
    "import seaborn as sns                        # Graphing\n",
    "from collections import defaultdict, Counter # Utils\n",
    "sns.set(style=\"white\")                       # Tuning the style of charts\n",
    "import warnings                              # Disable some warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_limit = 10000\n",
    "keep_dates = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a DSS dataset as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset\n",
      "   Rows: 176\n",
      "   Columns: 21 (19 num, 2 cat, 0 date)\n"
     ]
    }
   ],
   "source": [
    "# Take a handle on the dataset\n",
    "mydataset = dataiku.Dataset(\"EXPLORATION_NOTIF_CONSEIL\")\n",
    "\n",
    "# Load the first lines.\n",
    "# You can also load random samples, limit yourself to some columns, or only load\n",
    "# data matching some filters.\n",
    "#\n",
    "# Please refer to the Dataiku Python API documentation for more information\n",
    "df = mydataset.get_dataframe(limit = dataset_limit)\n",
    "\n",
    "df_orig = df.copy()\n",
    "\n",
    "# Get the column names\n",
    "numerical_columns = list(df.select_dtypes(include=[np.number]).columns)\n",
    "categorical_columns = list(df.select_dtypes(include=[object]).columns)\n",
    "date_columns = list(df.select_dtypes(include=['<M8[ns]']).columns)\n",
    "\n",
    "# Print a quick summary of what we just loaded\n",
    "print \"Loaded dataset\"\n",
    "print \"   Rows: %s\" % df.shape[0]\n",
    "print \"   Columns: %s (%s num, %s cat, %s date)\" % (df.shape[1], \n",
    "                                                    len(numerical_columns), len(categorical_columns),\n",
    "                                                    len(date_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the data <a id=\"preprocessing\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the dates as features if requested by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['marg428','marg572','marg524','marg381','marg621','cible_marg381','groupe','traj_hors_flux_texte','Annee','cible_marg380']\n",
    "\n",
    "if keep_dates:\n",
    "    df[date_columns] = df[date_columns].astype(int)*1e-9\n",
    "else:\n",
    "    columns_to_drop.extend(date_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 21)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of the columns that contain too many unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_LIMIT_ABS = 200\n",
    "CAT_DROP_LIMIT_RATIO = 0.5\n",
    "for feature in categorical_columns:\n",
    "    nu = df[feature].nunique()\n",
    "    \n",
    "    if nu > DROP_LIMIT_ABS or nu > CAT_DROP_LIMIT_RATIO*df.shape[0]:\n",
    "        print \"Dropping feature %s with %s values\" % (feature, nu)\n",
    "        columns_to_drop.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the columns \n",
    "print \"Dropping the following columns: %s\" % columns_to_drop\n",
    "df = df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filling missing values\n",
    "numerical_columns = list(df.select_dtypes(include=[np.number]).columns)\n",
    "categorical_columns = list(df.select_dtypes(include=[object]).columns)\n",
    "# Use mean for numerical features\n",
    "for feature in numerical_columns:\n",
    "    v = df[feature].mean()\n",
    "    if np.isnan(v):\n",
    "        v = 0\n",
    "    print \"Filling %s with %s\" % (feature, v)\n",
    "    df[feature] = df[feature].fillna(v)\n",
    "    \n",
    "# Use mode for categorical features\n",
    "for feature in categorical_columns:\n",
    "    v = df[feature].value_counts().index[0]\n",
    "    df[feature] = df[feature].fillna(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical variables with more than that many values, we only keep the most frequent ones\n",
    "LIMIT_DUMMIES = 100\n",
    "\n",
    "# Only keep the top 100 values\n",
    "def select_dummy_values(train, features):\n",
    "    dummy_values = {}\n",
    "    for feature in features:\n",
    "        values = [\n",
    "            value\n",
    "            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n",
    "        ]\n",
    "        dummy_values[feature] = values\n",
    "    return dummy_values\n",
    "\n",
    "DUMMY_VALUES = select_dummy_values(df, [x for x in categorical_columns if not x in columns_to_drop])\n",
    "\n",
    "\n",
    "def dummy_encode_dataframe(df):\n",
    "    for (feature, dummy_values) in DUMMY_VALUES.items():\n",
    "        for dummy_value in dummy_values:\n",
    "            dummy_name = u'%s_value_%s' % (feature, dummy_value.decode('utf-8'))\n",
    "            df[dummy_name] = (df[feature] == dummy_value).astype(float)\n",
    "        del df[feature]\n",
    "        print 'Dummy-encoded feature %s' % feature\n",
    "\n",
    "dummy_encode_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we rescale the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"CERFRANCE\",inplace=True)\n",
    "#df.drop(columns=[\"cible_marg380\"])\n",
    "df.head()\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "#print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler().fit(X)\n",
    "X_std = ss.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the PCA <a id=\"pca\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's \"fit\" the PCA algorithm (in other words, let's compute the singular value decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
      "  svd_solver='full', tol=0.0, whiten=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "acp = PCA(svd_solver='full')\n",
    "print(acp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=X_std\n",
    "coord = acp.fit_transform(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acp.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Eigen Values and Screenplot\n",
    "#Explained Variance\n",
    "print(acp.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valeur corrigée\n",
    "n = X.shape[0]\n",
    "p = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Les valeurs propres des 10 composantes\n",
    "eigval=acp.singular_values_**2/n\n",
    "print(acp.singular_values_**2/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion de variance expliquée\n",
    "print(acp.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scree plot\n",
    "plt.plot(numpy.arange(1,p+1),eigval)\n",
    "plt.title(\"Scree plot\")\n",
    "plt.ylabel(\"Eigen values\")\n",
    "plt.xlabel(\"Factor number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cumul de variance expliquée\n",
    "plt.plot(numpy.arange(1,p+1),numpy.cumsum(acp.explained_variance_ratio_))\n",
    "plt.title(\"Explained variance vs. # of factors\")\n",
    "plt.ylabel(\"Cumsum explained variance ratio\")\n",
    "plt.xlabel(\"Factor number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Détermination du nombre de composantes\n",
    "# Nous allons nous intéressé aux 5 composantes qui permettent d'expliquer 90 pour cent de la variabilité. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Représentation des Cerfrances. utilisation des coordonnées factorielles. \n",
    "#positionnement des individus dans le premier plan\n",
    "fig, axes = plt.subplots(figsize=(12,12))\n",
    "axes.set_xlim(-6,6) #même limites en abscisse\n",
    "axes.set_ylim(-6,6) #et en ordonnée\n",
    "#placement des étiquettes des observations\n",
    "for i in range(n):\n",
    "     plt.annotate(X.index[i],(coord[i,0],coord[i,1]))\n",
    "#ajouter les axes\n",
    "plt.plot([-6,6],[0,0],color='silver',linestyle='-',linewidth=1)\n",
    "plt.plot([0,0],[-6,6],color='silver',linestyle='-',linewidth=1)\n",
    "#affichage\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#contribution des individus dans l'inertie totale\n",
    "di = numpy.sum(Z**2,axis=1)\n",
    "print(pd.DataFrame({'ID':X.index,'d_i':di}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cos2 = coord**2\n",
    "for j in range(p):\n",
    "     cos2[:,j] = cos2[:,j]/di\n",
    "print(pd.DataFrame({'id':X.index,'COS2_1':cos2[:,0],'COS2_2':cos2[:,1]}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#contributions aux axes\n",
    "ctr = coord**2\n",
    "for j in range(p):\n",
    "     ctr[:,j] = ctr[:,j]/(n*eigval[j])\n",
    "\n",
    "print(pd.DataFrame({'id':X.index,'CTR_1':ctr[:,0],'CTR_2':ctr[:,1],'CTR_3':ctr[:,2],'CTR_4':ctr[:,3],'CTR_5':ctr[:,4]}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(numpy.sum(ctr,axis=0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Représentation des variables \n",
    "print(acp.components_) ## Les facteurs sont en lignes et les variables en colonnes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigval=acp.singular_values_**2/n\n",
    "print(acp.singular_values_**2/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#racine carrée des valeurs propres\n",
    "sqrt_eigval = numpy.sqrt(eigval)\n",
    "print(sqrt_eigval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrélation des variables avec les axes\n",
    "corvar = numpy.zeros((p,p))\n",
    "for k in range(p):\n",
    "     corvar[:,k] = acp.components_[k,:] * sqrt_eigval[k]\n",
    "\n",
    "#afficher la matrice des corrélations variables x facteurs\n",
    "#print(corvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table de corrélation des variables avec les axes\n",
    "#on affiche pour les cinq premiers axes\n",
    "print(pd.DataFrame({'id':X.columns,'COR_1':corvar[:,0],'COR_2':corvar[:,1],'COR_3':corvar[:,2],'COR_4':corvar[:,3],'COR_5':corvar[:,4],'COR_6':corvar[:,5],'COR_7':corvar[:,6]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cercle des corrélations\n",
    "fig, axes = plt.subplots(figsize=(8,8))\n",
    "axes.set_xlim(-1,1)\n",
    "axes.set_ylim(-1,1)\n",
    "#affichage des étiquettes (noms des variables)\n",
    "for j in range(p):\n",
    "     plt.annotate(X.columns[j],(corvar[j,0],corvar[j,1]))\n",
    "\n",
    "#ajouter les axes\n",
    "plt.plot([-1,1],[0,0],color='silver',linestyle='-',linewidth=1)\n",
    "plt.plot([0,0],[-1,1],color='silver',linestyle='-',linewidth=1)\n",
    "#ajouter un cercle\n",
    "cercle = plt.Circle((0,0),1,color='blue',fill=False)\n",
    "axes.add_artist(cercle)\n",
    "#affichage\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cos2var = corvar**2\n",
    "print(pd.DataFrame({'id':X.columns,'COS2_1':cos2var[:,0],'COS2_2':cos2var[:,1]}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(cos2var[:,0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(ctrvar[:,0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(eigval)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ctrvar = cos2var\n",
    "for k in range(p):\n",
    "     ctrvar[:,k] = ctrvar[:,k]/eigval[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## La contribution des variables aux axes, on présente les 5 premiers axes. \n",
    "print(pd.DataFrame({'id':X.columns,'CTR_1':ctrvar[:,0],'CTR_2':ctrvar[:,1],'CTR_3':ctrvar[:,2],'CTR_4':ctrvar[:,3],'CTR_5':ctrvar[:,4]}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(numpy.sum(ctrvar,axis=0))"
   ]
  }
 ],
 "metadata": {
  "analyzedDataset": "EXPLORATION_NOTIF_CONSEIL",
  "creator": "admin",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
